{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic simplest version\n",
    "In this file, using tf-idf with SVM abd Naive Bayes.  Comparing 50/50 80/20 90/10 data splits.\n",
    "\n",
    "Working with nltk version 3.2.5 and scikit-learn version 0.20.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# SVM\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# NB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step by step reference, no pipeline\n",
    "\n",
    "# # count words\n",
    "# count_vect = CountVectorizer()\n",
    "# X_train_count = count_vect.fit_transform(X_train.text)\n",
    "# X_train_count.shape\n",
    "\n",
    "# # tf-idf\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# X_train_tfidf = tfidf_transformer.fit_transform(X_train_count)\n",
    "# X_train_tfidf.shape\n",
    "\n",
    "# # NB\n",
    "# clf = MultinomialNB().fit(X_train_tfidf, X_train.author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "Using pipelines to help speed things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = Pipeline([(\"vect\", CountVectorizer()),\n",
    "                        (\"tfidf\", TfidfTransformer()),\n",
    "                        (\"clf-svm\", SGDClassifier(loss = \"hinge\", penalty = \"l2\",\n",
    "                                                tol = 1e-3, \n",
    "                                                random_state = 42)),])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf = Pipeline([(\"vect\", CountVectorizer()),\n",
    "                    (\"tfidf\", TfidfTransformer()),\n",
    "                    (\"clf- nb\", MultinomialNB(),)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = Pipeline([(\"vect\", CountVectorizer()),\n",
    "                    (\"tfidf\", TfidfTransformer()),\n",
    "                    (\"clf-rf\", RandomForestClassifier(random_state = 42, n_estimators = 10),)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate pipelines\n",
    "Create lists and a dictionary to help iterate through the fit and predict parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [svm_clf, nb_clf, rf_clf]\n",
    "pipe_dict = {0: \"Support Vector Machine\", 1: \"Naive Bayes\", 2: \"Random Forest\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features, using bag of words model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text author\n",
      "0  id26305  This process, however, afforded me no means of...    EAP\n",
      "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
      "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
      "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
      "4  id12958  Finding nothing else, not even gold, the Super...    HPL\n"
     ]
    }
   ],
   "source": [
    "dataPath = os.path.join(os.getcwd(), \"data\", \"\")\n",
    "train = pd.read_csv(dataPath + \"train.csv\")\n",
    "if(verbose):\n",
    "    print(train.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of X50_train  (9789,)\n",
      "Shape of y50_train  (9789,)\n",
      "Shape of X50_test  (9790,)\n",
      "Shape of y50_test  (9790,)\n",
      "\n",
      "Shape of X80_train  (15663,)\n",
      "Shape of y20_train  (15663,)\n",
      "Shape of X80_test  (3916,)\n",
      "Shape of y20_test  (3916,)\n",
      "\n",
      "Shape of X90_train  (17621,)\n",
      "Shape of y10_train  (17621,)\n",
      "Shape of X90_train  (17621,)\n",
      "Shape of y10_train  (17621,)\n"
     ]
    }
   ],
   "source": [
    "X50_train, X50_test, y50_train, y50_test = train_test_split(train.text, train.author, test_size = .50)\n",
    "X80_train, X80_test, y20_train, y20_test = train_test_split(train.text, train.author, test_size = .20) \n",
    "X90_train, X90_test, y10_train, y10_test = train_test_split(train.text, train.author, test_size = .10)\n",
    "\n",
    "if(verbose):\n",
    "    print(\"\\nShape of X50_train \", X50_train.shape)\n",
    "    print(\"Shape of y50_train \", y50_train.shape)\n",
    "    print(\"Shape of X50_test \", X50_test.shape)\n",
    "    print(\"Shape of y50_test \", y50_test.shape)\n",
    "    \n",
    "    print(\"\\nShape of X80_train \", X80_train.shape)\n",
    "    print(\"Shape of y20_train \", y20_train.shape)\n",
    "    print(\"Shape of X80_test \", X80_test.shape)\n",
    "    print(\"Shape of y20_test \", y20_test.shape)\n",
    "    \n",
    "    print(\"\\nShape of X90_train \", X90_train.shape)\n",
    "    print(\"Shape of y10_train \", y10_train.shape)\n",
    "    print(\"Shape of X90_train \", X90_train.shape)\n",
    "    print(\"Shape of y10_train \", y10_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "X_train.append(X50_train)\n",
    "y_train.append(y50_train)\n",
    "X_test.append(X50_test)\n",
    "y_test.append(y50_test)\n",
    "    \n",
    "X_train.append(X80_train)\n",
    "y_train.append(y20_train)\n",
    "X_test.append(X80_test)\n",
    "y_test.append(y20_test)\n",
    "    \n",
    "X_train.append(X90_train)\n",
    "y_train.append(y10_train)\n",
    "X_test.append(X90_train)\n",
    "y_test.append(y10_train)\n",
    "\n",
    "train_dict = {0: \"50/50 split\", 1: \"80/20 split\", 2: \"90/10 split\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking into  50/50 split\n",
      "\n",
      "Estimator: Support Vector Machine\n",
      "Test set accuracy score: 0.810 \n",
      "Average prediction: 0.810\n",
      "['HPL' 'EAP' 'EAP' ... 'MWS' 'EAP' 'EAP']\n",
      "\n",
      "Estimator: Naive Bayes\n",
      "Test set accuracy score: 0.780 \n",
      "Average prediction: 0.780\n",
      "['HPL' 'EAP' 'EAP' ... 'MWS' 'EAP' 'EAP']\n",
      "\n",
      "Estimator: Random Forest\n",
      "Test set accuracy score: 0.604 \n",
      "Average prediction: 0.604\n",
      "['MWS' 'EAP' 'HPL' ... 'EAP' 'EAP' 'EAP']\n",
      "\n",
      "Looking into  80/20 split\n",
      "\n",
      "Estimator: Support Vector Machine\n",
      "Test set accuracy score: 0.826 \n",
      "Average prediction: 0.826\n",
      "['EAP' 'MWS' 'MWS' ... 'HPL' 'MWS' 'EAP']\n",
      "\n",
      "Estimator: Naive Bayes\n",
      "Test set accuracy score: 0.806 \n",
      "Average prediction: 0.806\n",
      "['EAP' 'MWS' 'EAP' ... 'HPL' 'EAP' 'EAP']\n",
      "\n",
      "Estimator: Random Forest\n",
      "Test set accuracy score: 0.620 \n",
      "Average prediction: 0.620\n",
      "['EAP' 'MWS' 'HPL' ... 'EAP' 'MWS' 'EAP']\n",
      "\n",
      "Looking into  90/10 split\n",
      "\n",
      "Estimator: Support Vector Machine\n",
      "Test set accuracy score: 0.927 \n",
      "Average prediction: 0.927\n",
      "['EAP' 'MWS' 'MWS' ... 'HPL' 'MWS' 'MWS']\n",
      "\n",
      "Estimator: Naive Bayes\n",
      "Test set accuracy score: 0.900 \n",
      "Average prediction: 0.900\n",
      "['EAP' 'MWS' 'MWS' ... 'HPL' 'MWS' 'MWS']\n",
      "\n",
      "Estimator: Random Forest\n",
      "Test set accuracy score: 0.988 \n",
      "Average prediction: 0.988\n",
      "['MWS' 'MWS' 'MWS' ... 'HPL' 'MWS' 'MWS']\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(X_train):\n",
    "    \n",
    "    print(\"\\nLooking into \",  train_dict[i])\n",
    "    \n",
    "    for idx, pl in enumerate(pipelines):\n",
    "        print(\"\\nEstimator:\",  pipe_dict[idx])\n",
    "        pl.fit(X_train[i], y_train[i])\n",
    "        y_pred = pl.predict(X_test[i])\n",
    "        print('Test set accuracy score: %.3f ' % accuracy_score(y_test[i], y_pred))\n",
    "        # end inner for over pipeline\n",
    "    #end outer for over data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "Check nltk and sklearn versions: https://stackoverflow.com/questions/28501072/how-to-check-which-version-of-nltk-scikit-learn-installed\n",
    "Pipeline help: https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
